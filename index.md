---
layout: home
title: Yaowen Ye
---

# Research Interests

I am interested in ensuring reliable human supervision for complex AI systems that produce complex outputs. Some questions I am thinking about recently are:
- How can we design human-AI collaboration mechanisms to amplify human oversight so that increase in human effort is minimized even as AI's complexity and compute scale?
- When human feedback is imperfect, how can AIs learn to do better than humans without replicating their errors?
- Can models develop good values purely based on reasoning so that the impact of human supervision errors can be minimized?

In the past, I also worked on
- Cognitive reasoning: How can we explain humans' reasoning process such as intuitive physics and abductive reasoning? How do these explanations inspire understanding of AI?
- Learning on graphs: How can we better model graph data for applications like recommender systems? How can we ensure these models are designed equally for different users and responsibly for real-world deployment?
