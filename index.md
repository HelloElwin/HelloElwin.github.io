---
layout: home
title: Yaowen Ye
---

# Research Interests

I am interested in making human oversight of AI systems reliable on hard tasks and efficient for complex outputs. Some problems I am thinking about recently are:
- **Alignment under unreliable supervision.** As humans inevitably make mistakes due to limited capabilities or cognitive and social biases, we need methods that ensure reliable alignment even with systematically flawed supervision. 
- **Scaling human oversight to interacting AI systems.** AI systems will be deployed at massive scale due to their growing capabilities and cost-efficiency. We need algorithms and tools that help humans efficiently understand the complex outputs of multiple interacting AI systems.
- **Scaling laws for human oversight.** We need better evaluation of oversight methods, instead of merely measuring accuracy. I aim to develop empirically-grounded scaling laws that helps us predict human resource requirements for reliably supervising an AI system on a specific task, hence revealing which oversight methods may remain practical in the future.

In the past, I also worked on
- Cognitive reasoning: How can we explain humans' reasoning process such as intuitive physics and abductive reasoning? How do these explanations inspire understanding of AI?
- Learning on graphs: How can we better model graph data for applications like recommender systems? How can we ensure these models are designed equally for different users and responsibly for real-world deployment?
